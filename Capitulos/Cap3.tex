\chapter{Inferencia y Predicci\'on}
\section{Introducci\'on}
En el cap\'itulo anterior se describi\'o el modelo general de probabilidad que describe el proceso de duraciones y costos en un padecimiento cr\'onico degenerativo, por lo que el siguiente paso ser\'ia hacer inferencia sobre el mismo. En este tipo de modelos, el objetivo de hacer inferencia es predecir futuras observaciones en base a los datos ya observados. En este cap\'itulo se sentar\'an las bases para realizar esta inferencia.\\
\\
El primer paso para realizar inferencia es la construcci\'on de la funci\'on de verosimilitud, en este caso extendida a las variables latentes y a los par\'ametros correspondientes a sus distribuciones de todos los individuos de la poblaci\'on. Una vez que se determinaron las funciones de verosimilitud, se analizan los m\'etodos de estimaci\'on que se podr\'ian usar para la predicci\'on de futuras observaciones. %De este modo se puede completar la parte te\'orica de este trabajo de investigaci\'on.
\section{Verosimilitud Extendida}
Como especificado en la secci\'on anterior, una vez que el modelo de probabilidad describe de manera precisa los datos del problema podemos empezar a hacer inferencia sobre observaciones futuras. La base sobre la que se puede hacer predicci\'on en base a los datos ya observados es la funci\'on de verosimilitud definida como la funci\'on de de distribuci\'on conjunta de los datos.\\
\\
As\'i, para un modelo de probabilidad donde los datos se caracterizan mediante $p(x_j|\theta)$; la funci\'on de verosimilitud seg\'un \cite{held2014applied}, se define como
\begin{defi}
La funci\'on de verosimilitud $V(\theta)$ es la funci\'on masa o la funci\'on de densidad de los datos observados $x$, entendidos en funci\'on del par\'ametro desconocido $\theta$.
\end{defi}
En este caso las variables observables se definen en funci\'on de las variables latentes, estas a su vez se describen en funci\'on de sus par\'ametros. De esto se desprende la noci\'on de verosimilitud extendida para incluir las variables latentes. Como se especifica en \cite{pitt2002constructing}, la construcci\'on de la funci\'on de verosimilitud resulta sencilla, incluso intuitiva. Sin embargo, la estimaci\'on de los par\'ametros mediante m\'axima verosimilitud no es tan sencilla pues no tiene una soluci\'on que se pueda expresar de manera anal\'itica cerrada. Usando esta construcci\'on, se escribe una funci\'on de verosimilitud para el modelo general de probabilidad de duraciones y costos de un solo individuo\\
\begin{eqnarray}
V(\{\theta_j\},\{\gamma_j\},\{d_j\},\{c_j\}) &=& f(d_1|\theta_1)f(c_1|d_1,\gamma_1)f(\gamma_1) f(\theta_1)\times\nonumber\\
& \times & \prod_{j=2}^{N(t)} f(d_j|\theta_j)f(\theta_j|d_{j-1})f(c_j|d_j,\gamma_j)f(\gamma_j|c_{j-1})\nonumber
\end{eqnarray}
Para poder calcular la funci\'on de verosimilitud que permite hacer inferencia, es necesario conocer las distribuciones de las variables latentes con base en las observaciones anteriores para ambas variables observables, duraciones y costos.\\
\\Para la primera variable observable se toma en cuenta la relaci\'on 
\[f_{\theta|d}(\theta|d)\propto f_{d|\theta}(d|\theta)f(\theta)\]
y que $d|\theta \sim Gamma(d|\alpha_d,\theta)$ y $\theta \sim Gamma(\theta|\alpha_\theta,\beta_\theta)$.

\begin{eqnarray*}
f_{\theta|d}(\theta|d) &\propto& \frac{\theta^{\alpha_d}}{\Gamma(\alpha_d)}\quad d^{\alpha_d-1} e^{-\{\theta d\}} \times \frac{\beta_\theta^{\alpha_\theta}}{\Gamma(\alpha_\theta)}\quad \theta^{\alpha_\theta-1} e^{-\{-\beta_\theta \theta\}}\\
&=&\frac{\beta_\theta^{\alpha_\theta}}{\Gamma(\alpha_d)\Gamma(\alpha_\theta)} \quad d^{\alpha_d-1} \quad\theta^{\alpha_d+\alpha_\theta-1} \quad e^{\{-\theta(d+\beta_\theta)\}}\\
\\
&\propto& \theta^{\alpha_d+\alpha_\theta-1} \quad e^{\{-\theta(d+\beta_\theta)\}}\\
&\Rightarrow& \theta|d \sim Gamma(\alpha_d+\alpha_\theta,d+\beta_\theta)
\end{eqnarray*}
\\
Para la variable de duraciones, la distribuci\'on de la variable latente que depende de la observaci\'on se puede expresar de una manera anal\'itica cerrada como la distribuci\'on Gamma. An\'alogamente, para la variable de costos se vuelve a tomar en cuenta la misma relaci\'on y las distribuciones 
\[c|d,\gamma \sim Weibull(c|d,\gamma) \qquad \gamma \sim InvGamma(\gamma|\alpha_\gamma,\beta_\gamma)\]
\begin{eqnarray*}
f_{\gamma|d,c}(\gamma|d,c) &\propto& \frac{d}{\gamma^d}\quad c^{d-1} e^{\{-(\frac{c}{\gamma})^d\}} \times \frac{\beta_\gamma^{\alpha_\gamma}}{\Gamma(\alpha_\gamma)}\quad (\frac{1}{\gamma})^{\alpha_\gamma+1} \quad e^{\{-(\frac{\beta_\gamma}{\gamma})\}}\\
&=&\frac{d\beta_\gamma^{\alpha_\gamma} c^{d-1}}{\Gamma(\alpha_\gamma)}\quad (\frac{1}{\gamma})^{d+\alpha_\gamma+1}\quad e^{-((\frac{\beta_\gamma}{\gamma})+(\frac{c}{\gamma})^d)}\\
&\propto&(\frac{1}{\gamma})^{d+\alpha_\gamma+1}\quad e^{-((\frac{\beta_\gamma}{\gamma})+(\frac{c}{\gamma})^d)}
\end{eqnarray*}
\\
Para la variable de costos, la distribuci\'on de la variable latente seg\'un la observaci\'on anterior no tiene una forma anal\'itica cerrada como distribuci\'on, sin embargo, el kernel se puede simular con un slice sampler; este m\'etodo se explicar\'a con detalle en el ap\'endice. \\
\\
Una vez que queda definidas las distribuciones de las variables latentes con base en las observaciones anteriores es importante notar que los p\'arametros de las distribuciones ($\alpha_d, \alpha_\theta, \beta_\theta, \alpha_\gamma, \beta_\gamma$), por constucci\'on, no dependen de la realizaci\'on; por lo que al modelo jer\'arquico establecido en la Figura 2.4 se agrega otro nivel. En la siguiente figura se muestra una representaci\'on del nuevo modelo jer\'arquico, donde los par\'ametros definen a las variables latentes y \'estas a su vez, mediante las relaciones que ya establecimos, definen a las observaciones. Esta figura representa la estructura jer\'arquica del modelo de probabilidad del individuo $i$.\\
\begin{figure}[h!]
\begin{center}
\begin{picture}(200,80)
\put(60,80){$\alpha_d$}
\put(80,80){$\alpha_\theta$}
\put(100,80){$\beta_\theta$}
\put(120,80){$\alpha_\gamma$}
\put(140,80){$\beta_\gamma$}
\put(60,75){\vector(-1,-1){30}}
\put(140,75){\vector(1,-1){30}}
\put(5,40){$\theta_1,\gamma_1$}
\put(20,35){\vector(0,-1){20}}
\put(170,35){\vector(0,-1){20}}
\put(50,40){$\ldots$}
\put(100,40){$\ldots$}
\put(130,40){$\ldots$}
\put(160,40){$\theta_t,\gamma_t$}
\put(5,5){$d_1,c_1$}
\put(50,5){$\ldots$}
\put(100,5){$\ldots$}
\put(130,5){$\ldots$}
\put(160,5){$d_t,c_t$}
\end{picture}
\end{center}
\caption{Modelo jer\'arquico de los par\'ametros, variables latentes y observaciones del individuo $i$.}
\end{figure}
\\
De este modo, las distribuciones que resultan de los c\'alculos anteriores son la llave que se necesita para empezar a hacer inferencia, tomando estas distrbuciones se redefine la verosimilitud de un solo individuo como
\begin{eqnarray*}
&&V(\{\alpha_d,\alpha_\theta,\beta_\theta,\alpha_\gamma,\beta_\gamma\},\{\theta_i,\gamma_i\}_{j=1}^{N(t)}|\{d_j,c_j\}_{i=1}^I)=\\ 
&&\prod_{j=1}^{N(t)} Gamma(d_j|\alpha_d,\theta_j) Gamma(\theta_j|\alpha_d+\alpha_\theta,d_{j-1}+\beta_\theta)\times \\
&&Weibull(c_j|d_j,\gamma_j)(\frac{1}{\gamma})^{d_{j-1}+\alpha_\gamma+1}e^\{-(\frac{\beta_\gamma}{\gamma}+(\frac{c_{j-1}}{\gamma})^{d_j})\} \times \\
&&\times \pi(\alpha_d)\pi(\alpha_\theta)\pi(\beta_\theta)\pi(\alpha_\gamma)\pi(\beta_\gamma)
\end{eqnarray*}
Donde para las variables latentes asociada a las duraciones,
\begin{eqnarray*}
\pi(\theta_j|\alpha_d,\alpha_\theta,\beta_\theta)&\propto& Gamma(d_j|\alpha_d,\theta_j)\times Gamma(\theta_j|\alpha_d+\alpha_\theta,d_{j-1}+\beta_\theta)\\
\\
&=&\frac{\theta_j^{\alpha_d}}{\Gamma(\alpha_d)} d_j^{\alpha_d-1} e^{\{-\theta_jd_j\}}\frac{(d_{j-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}}{\Gamma(\alpha_d+\alpha_\theta)}\theta_j^{\alpha_d+\alpha_\theta}e^{\{d_{j-1}+\beta_\theta\}}\\
\\
&=&\frac{d_j^{\alpha_d-1}(d_{j-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}}{\Gamma(\alpha_d)\Gamma(\alpha_d+\alpha_\theta)} \theta_j^{2\alpha_d+\alpha_\theta-1}e^{\{-\theta_j(d_{j-1}+d_j+\beta_\theta)\}}\\
\\
&\propto& \theta_j^{2\alpha_d+\alpha_\theta-1}e^{\{-\theta_j(d_{j-1}+d_j+\beta_\theta)\}}\\
\\
&\Rightarrow& \theta_j \sim Gamma(2\alpha_d+\alpha_\theta,d_j+d_{j-1}+\beta_\theta)
\end{eqnarray*}
Y para la variable latente asociada a los costos,
\begin{eqnarray*}
\pi(\gamma_j|\alpha_\gamma,\beta_\gamma,d_j,c_{j-1})&\propto& Weibull(c_j|d_j,\gamma_j)\times (\frac{1}{\gamma_j})^{d_j +\alpha_\gamma+1}e^{\{-(\frac{\beta_\gamma}{\gamma_j}+(\frac{c_{j-1}}{\gamma_j})^{d_j}\}}\\
&=&\frac{d_j}{\gamma_j^{d_j}}c_j^{d_j-1}e^{\{-(\frac{c_j}{\gamma_j})\}}(\frac{1}{\gamma_j})^{d_j+\alpha_\gamma+1}e^{\{-(\frac{\beta_\gamma}{\gamma_j}+(\frac{c_{j-1}}{\gamma_j})^{d_j})\}}\\
&=&d_j c_j^{d_j-1}(\frac{1}{\gamma_j})^{2d_j+\alpha_\gamma+1}e^{\{-(\frac{\beta_\gamma}{\gamma_j}+(\frac{c_{j-1}}{\gamma_j})^{d_j}+(\frac{c_j}{\gamma_j})^{d_j})\}}\\
&\propto&(\frac{1}{\gamma_j})^{2d_j+\alpha_\gamma+1}e^{\{-(\frac{\beta_\gamma}{\gamma_j}+(\frac{c_{j-1}+c_j}{\gamma_j})^{d_j}\}}
\end{eqnarray*}
De manera an\'aloga, las distribuciones que corresponden a los par\'ametros para un solo individuo $i$,
\begin{eqnarray*}
\pi(\alpha_d|...)&\propto& \prod_{j=1}^{N(t_i)} Gamma(\theta_j|\alpha_d+\alpha_\theta,d_{j-1}+\beta_\theta)\times Gamma(\alpha_d|\alpha_0,\beta_0)\\
\\
&=&\prod_{j=1}^{N(t_i)}\frac{(d_{j-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}}{\Gamma(\alpha_d+\alpha_\theta)} \theta_j^{\alpha_d+\alpha_\theta-1} e^{\{-\theta_j(d_{j-1}+\beta_\theta)\}}\frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \alpha_d^{\alpha_0-1}e^{\{-\alpha_d\beta_0\}}\\
\\
&\propto&\prod_{j=1}^{N(t_i)}\frac{(d_{j-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}}{\Gamma(\alpha_d+\alpha_\theta)} \theta_j^{\alpha_d}\alpha_d^{\alpha_0-1}e^{\{-\alpha_d\beta_0\}}
\end{eqnarray*}
\\
\begin{eqnarray*}
\pi(\alpha_\theta|...)&\propto&\prod_{j=1}^{N(t_i)} Gamma(\theta_j|\alpha_d+\alpha_\theta,d_{j-1}+\beta_\theta)\times Gamma(\alpha_\theta|\alpha_0,\beta_0)\\
\\
&=&\prod_{j=1}^{N(t_i)}\frac{(d_{j-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}}{\Gamma(\alpha_d+\alpha_\theta)} \theta_j^{\alpha_d+\alpha_\theta-1} e^{\{-\theta_j(d_{j-1}+\beta_\theta)\}}\frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \alpha_\theta^{\alpha_0-1}e^{\{-\alpha_\theta\beta_0\}}\\
\\
&\propto& \prod_{j=1}^{N(t_i)}\frac{(d_{j-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}}{\Gamma(\alpha_d+\alpha_\theta)} \theta_j^{\alpha_\theta}\alpha_\theta^{\alpha_0-1}e^{\{-\alpha_\theta\beta_0\}}
\end{eqnarray*}
\\
\begin{eqnarray*}
\pi(\beta_\theta|...)&\propto&\prod_{j=1}^{N(t_i)} Gamma(\theta_i|\alpha_d+\alpha_\theta,d_{i-1}+\beta_\theta)\times Gamma(\beta_\theta|\alpha_0,\beta_0)\\
\\
&=&\prod_{i=2}^{N(t)}\frac{(d_{i-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}}{\Gamma(\alpha_d+\alpha_\theta)} \theta_i^{\alpha_d+\alpha_\theta-1} e^{\{-\theta_i(d_{i-1}+\beta_\theta)\}}\frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \beta_\theta^{\alpha_0-1}e^{\{-\beta_\theta\beta_0\}}\\
\\
&\propto&\prod_{j=1}^{N(t)}(d_{j-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}\beta_\theta^{\alpha_0-1} e^{\{-\beta_\theta(\theta_j+\beta_0)\}}
\end{eqnarray*}
\\
\begin{eqnarray*}
\pi(\alpha_\gamma|...)&\propto&\prod_{j=1}^{N(t)} \pi(\gamma_i|\alpha_\gamma,\beta_\gamma,d_i,c_{j-1})\times Gamma(\alpha_\gamma|\alpha_0,\beta_0)\\
&=&\prod_{j=1}^{N(t)}(\frac{1}{\gamma_i})^{d_j+\alpha_\gamma+1}e^{\{-(\frac{\beta_\gamma}{\gamma_j}+(\frac{c_{j-1}}{\gamma_j})^{d_j})\}}\frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \alpha_\gamma^{\alpha_0-1}e^{\{-\alpha_\gamma\beta_0\}}\\
\\
&\propto&\prod_{j=1}^{N(t)} (\frac{1}{\gamma_j})^{\alpha_\gamma}\alpha_\gamma^{\alpha_0-1}e^{\{-\alpha_\gamma\beta_0\}}
\end{eqnarray*}
\\
\begin{eqnarray*}
\pi(\beta_\gamma|...)&\propto&\prod_{i=2}^{N(t)} \pi(\gamma_i|\alpha_\gamma,\beta_\gamma,d_i,c_{i-1})\times Gamma(\beta_\gamma|\alpha_0,\beta_0)\\
\\
&=&\prod_{i=2}^{N(t)}(\frac{1}{\gamma_i})^{d_i+\alpha_\gamma+1}e^{\{-(\frac{\beta_\gamma}{\gamma_i}+(\frac{c_{i-1}}{\gamma_i})^{d_i})\}}\frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)} \beta_\gamma^{\alpha_0-1}e^{\{-\beta_\gamma\beta_0\}}\\
&\propto&\beta_\gamma^{\alpha_0-1}e^{\{-\beta_\gamma(\frac{1}{\gamma_i}+\beta_0)\}}\\
\\
&\Rightarrow& \beta_\gamma \sim Gamma(\alpha_0,\frac{1}{\gamma_i}+\beta_0)
\end{eqnarray*}
Los par\'ametros a estimar son aquellos correspondientes a las variables latentes, cuyo kernel se estima para cada momento $j$ ($j=1,...,N(t)$), y a las variables observables del modelo. Las verosimilitudes mostradas para los par\'ametros deben ser extendidas para incluir toda la poblaci\'on muestreada, de este modo,%Dado el uso de las variables latentes no existir\'a un \'optimo global para las variables, raz\'on por la cual se requiere el uso de m\'etodos num\'ericos no solo para la estimaci\'on de los par\'ametros y variables latentes sino para analizar las distribuciones que no tienen expresi\'on anal\'itica cerrada, como aquellas de los par\'ametros $\alpha_d,\alpha_\theta,\alpha_\gamma$ y la variable latente $\gamma_i$.\\
\begin{eqnarray*}
\pi(\alpha_d|(\theta_{ij})_{i=1}^I ._{j=1}^{N(t_i)})&\propto & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} \pi(\alpha_d|\theta_{ij},...)\\
&\propto & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} \frac{(d_{ij-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}}{\Gamma(\alpha_d+\alpha_\theta)} \theta_{ij}^{\alpha_d}\alpha_d^{\alpha_0-1}e^{\{-\alpha_d\beta_0\}}\\
&\propto & (\frac{\alpha_d^{\alpha_0 -1}}{\Gamma(\alpha_d+\alpha_\theta)})^{\sum_{i=1}^I N(t_i)} (\sum_{i=1}^I N(t_i) e^{\{-\alpha_d\beta_0\}}) \times \\
&\times & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} (d_{ij-1}+\beta_\theta)^{\alpha_\theta+\alpha_d} \theta_{ij}^{\alpha_d}
\end{eqnarray*}
\\
\begin{eqnarray*}
\pi(\alpha_\theta|(\theta_{ij})_{i=1}^I ._{j=1}^{N(t_i)})&\propto & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} \pi(\alpha_\theta|\theta_{ij},...)\\
&\propto &\prod_{i=1}^I \prod_{j=1}^{N(t_i)} \frac{(d_{ij-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}}{\Gamma(\alpha_d+\alpha_\theta)} \theta_{ij}^{\alpha_\theta}\alpha_\theta^{\alpha_0-1}e^{\{-\alpha_\theta\beta_0\}}\\
&\propto & (\frac{\alpha_\theta^{\alpha_0 -1}}{\Gamma(\alpha_d+\alpha_\theta)})^{\sum_{i=1}^I N(t_i)} (\sum_{i=1}^I N(t_i) e^{\{-\alpha_\theta\beta_0\}}) \times \\
&\times & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} (d_{ij-1}+\beta_\theta)^{\alpha_\theta+\alpha_d} \theta_{ij}^{\alpha_\theta}
\end{eqnarray*}
\\
\begin{eqnarray*}
\pi(\beta_\theta|(\theta_{ij})_{i=1}^I ._{j=1}^{N(t_i)})&\propto & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} \pi(\beta_\theta|\theta_{ij},...)\\
&\propto &\prod_{i=1}^I \prod_{j=1}^{N(t_i)} (d_{ij-1}+\beta_\theta)^{\alpha_d+\alpha_\theta}\beta_\theta^{\alpha_0-1} e^{\{-\beta_\theta(\theta_ij+\beta_0)\}}\\
&\propto & (\beta_\theta^{\alpha_0-1})^{\sum_{i=1}^I N(t_i)} e^{\{-\beta_\theta(\sum_{i=1}^I \sum_{j=1}^{N(t_i)}(\theta_ij+\beta_0))\}} \times \\
&\times & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} (d_{ij-1}+\beta_\theta)^{\alpha_\theta+\alpha_d}
\end{eqnarray*}
\\
\begin{eqnarray*}
\pi(\alpha_\gamma|(\gamma_{ij})_{i=1}^I ._{j=1}^{N(t_i)})&\propto & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} \pi(\alpha_\gamma|\gamma_{ij},...)\\
&\propto & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} (\frac{1}{\gamma_ij})^{\alpha_\gamma} \alpha_\gamma^{\alpha_0-1}e^{\{-\alpha_\gamma\beta_0\}}\\
&\propto & (\alpha_\gamma^{\alpha_0-1})^{\sum_{i=1}^I N(t_i)} (\sum_{i=1}^I N(t_i) e^{\{-\alpha_\gamma \beta_0\}}) \times \\
&\times & \prod_{i=1}^I \prod_{j=1}^{N(t_i)}(\frac{1}{\gamma_ij})^{\alpha_\gamma}
\end{eqnarray*}
\\
\begin{eqnarray*}
\pi(\beta_\gamma|(\theta_{ij})_{i=1}^I ._{j=1}^{N(t_i)})&\propto & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} \pi(\beta_\gamma|\gamma_{ij},...)\\
&\propto & \prod_{i=1}^I \prod_{j=1}^{N(t_i)} \beta_\gamma^{\alpha_0-1}e^{\{-\beta_\gamma(\frac{1}{\gamma_{ij}}+\beta_0)\}}\\
&\propto & (\beta_\gamma^{\alpha_0-1})^{\sum_{i=1}^I N(t_i)} e^{\{-\beta_\gamma(\sum_{i=1}^I \sum_{j=1}^{N(t_i)}\frac{1}{\gamma_{ij}}+\beta_0)\}}
\end{eqnarray*}
Es importante mencionar que estas distribuciones que no tienen forma anal\'itica cerrada son funciones log-c\'oncavas. Esta propiedad se explorar\'a con mayor detalle en la siguiente secci\'on.
\section{Log-concavidad en las funciones de distribuci\'on}
Las funciones log-c\'oncavas son, de acuerdo con \cite{bagnoli2005log}, funciones que se grafican con una curva c\'oncava en los n\'umeros reales positivos y cuyo logaritmo es tambi\'en una funci\'on c\'oncava; o bien, seg\'un \cite{an1996log} un vector aleatorio se distribuye de manera log-c\'oncava  si es logaritmo de la distribuci\'on de densidad es c\'oncavo en su soporte, es decir
\begin{defi}
Un vector de variables aleatorias, $X$, est\'a distribuido de manera log-c\'oncava si para cada $x_1, x_2 \in \Omega$ y cualquiera $\lambda \in [0,1]$,
\[f(\lambda x_1+(1-\lambda)x_2) \geq [f(x_1)]^\lambda [f(x_2)]^{1-\lambda}\] 
\end{defi}

Estas funciones tienen el siguiente teorema,
\begin{teo}
Sea $F$ sea una funci\'on doblemente diferenciable que toma valores positivos con soporte en $(a,b)$ y sea $t$ una funci\'on doblemente diferenciable y monot\'onica que va de $(a',b')$ a $(a,b)=(t(a'),t(b'))$. Se define la funci\'on $\hat{F}$ con soporte en $(a',b')$ para toda $x \in (a',b'), \quad \hat{F}(x)=F(t(x))$. Si F es log-c\'oncava y t una funci\'on c\'oncava, entonces $\hat{F}$ es log-c\'oncava.
\end{teo}
Este teorema tiene el siguiente corolario,
\begin{cor}
Sea F una funci\'on con soporte en (a,b). Sea $t$ una transformaci\'on lineal de la l\'inea real hacia si misma, y se define una funci\'on $\hat{F}$ con soporte en $(t(a),t(b))$ tal que $\hat{F}(x)=F(t(x))$. Si F es log-c\'oncava entonces $\hat{F}$ es log-c\'oncava. 
\end{cor}
Seg\'un \cite{bagnoli2005log} y \cite{an1996log}, la distribuci\'on Weibull cumple con las caracter\'isticas de log-concavidad si su par\'ametro de forma es mayor o igual a uno, para el caso de la distribuci\'on de los costos este par\'ametro corresponde a la duraci\'on, que por definici\'on es mayor a uno. Tambi\'en la distribuci\'on Gamma de $\alpha_d,\alpha_\theta, \alpha_\gamma$ debe tener el par\'ametro de forma mayor a uno, como sucede para la distribuci\'on Weibull, esta condici\'on se cumple por construcci\'on. Una de las propiedades de las distribuciones log-c\'oncavas es,
\begin{prop}
Sea $X$ una variable aleatoria cuya funci\'on de densidad $f(x)$ es log-c\'oncava. Entonces, para cada $\alpha \neq 0$, la variable aleatoria $Y=\alpha X+\beta$ es log-c\'oncava.
\end{prop}
Es decir, que toda transformaci\'on lineal de la variable aleatoria no afecta su propiedad de log-concavidad. En el contexto de este trabajo de investigaci\'on, es importante la proposici\'on que estipula \cite{an1996log} para las distribuciones multivariadas,
\begin{prop}
Sea $X = (X_1,...,X_k)$. Si las $X_i$'s son independientes y cada una de ellas tiene una funci\'on de densidad log-c\'oncava, entonces su densidad conjunta tambi\'en es log-c\'oncava.
\end{prop}
Esto quiere decir que si podemos demostrar log-concavidad para una de las distribuciones, esta propiedad se extiende a la distribuci\'on conjunta. De este modo aseguramos la log-concavidad para estas distribuciones que no tienen una f\'ormula anal\'itica cerrada y que se muestrearan de manera previa a la estimaci\'on de par\'ametros para el modelo general de probabilidad.
\section{Inferencia en modelos con variables latentes.}
Una vez que se especifican las distribuciones de los par\'ametros a estimar y las propiedades de los mismos, se necesitan m\'etodos que los puedan estimar. \cite{pitt2002constructing} especifica que la estimaci\'on de m\'axima verosimilitud puede resolverse mediante el algoritmo EM, aunque tambi\'en, debido a que las densidades son dos condicionales de la densidad conjunta puede ligarse con el Muestreador Gibbs.\\
\\
El algoritmo EM es un algoritmo para calcular el estimador de m\'axima verosimilitud, que de acuerdo con \cite{held2014applied}, se define como
\begin{defi}
El Estimador de M\'axima Verosimilitud (EMV) $\hat{\theta}_{MV}$ del par\'ametro $\theta$ se obtiene maximizando la funci\'on de verosimilitud.
\begin{align*}
\hat{\theta}_{MV}=max_{\theta \in \Theta} L(\theta)
\end{align*} 
\end{defi}
Seg\'un \cite{dempster1977maximum} el algoritmo EM calcula el EML mediante iteraciones, cada iteraci\'on consiste en un paso d\'onde se calcula la esperanza y en otro  se maximiza la misma, de ah\'i el nombre de EM. Este algoritmo se relaciona con las variables latentes suponiendo dos variables $x$ y $y$ las cuales se relacionan $x \to y(x)$, donde $y$ son los datos observables.\\
\\
De este modo, an\'alogamente a lo expresado en el cap\'itulo anterior por \cite{pitt2002constructing}, se proponen las siguientes funciones de densidad $f(x|\phi)$ y $g(y|\phi)$; en las cuales, de acuerdo a \cite{dempster1977maximum} los datos completos (variables latentes) $f(x|\cdot)$ se relacionan con los datos incompletos (variables observadas) $g(y|\cdot)$ mediante
\begin{align*}
g(y|\phi)=\int_{\chi(y)} f(x|\phi)dx
\end{align*}
El algoritmo EM se dedica a encontrar un valor de $\phi$ que maximice $g(y|\phi)$ dada la $y$ observada usando la familia asociada de $f(x|\phi)$. Una de las caracterizaciones m\'as simples supone $\phi^{(p)}$ es el valor actual de $\phi$ despu\'es de $p$ iteraciones y $t(x)$ como el estad\'istico suficientes de los datos completos, es decir, el estimador de la variable latente; por lo que la siguiente iteraci\'on se puede desglosar en los siguientes dos pasos:
\begin{itemize}
\item Paso E: Estimar los estad\'isticos suficientes de los datos completos.
	\begin{align*}
	t^{(p)}=E[t(x)|y,\phi^{(p)}]
	\end{align*}
\item Paso M: Determinar $\phi^{(p+1)}$ como soluci\'on a la ecuaci\'on
	\begin{align*}
	E[t(x)|\phi]=t^{(p)}
	\end{align*}
\end{itemize}
Es decir, que si suponemos que $t^{(p)}$ es el estad\'istico suficiente calculado de $x$ observada en la distribuci\'on $f(x|\phi)$ entonces la ecuaci\'on definida en el Paso M se define como el EMV. Este concepto se hace general al definir la siguiente funci\'on
\begin{align*}
Q(\phi'|\phi)=E[log f(x|\phi')|y,\phi]
\end{align*}
Esta funci\'on se asume que existe para toda pareja $(\phi',\phi)$. Se define la iteraci\'on EM para $\phi^{(p)} \to \phi^{(p+1)}$,
\begin{itemize}
\item Calcular $Q(\phi,\phi^{(p)})$.
\item Determinar $\phi^{(p+1)}$ tal que maximice $Q(\phi,\phi^{(p)})$.
\end{itemize}
La idea central es tomar una $\phi'$ que maximice $log f(x|\phi)$, dado que esta distribuci\'on y su correspondiente logaritmo no necesariamente se conoce, se puede maximizar los datos observados y $\phi^{(p)}$.\\
\\
El algoritmo EM es muy \'util pues por su estructura iterativa puede dar resultados a modelos de probabilidad muy complejos, adem\'as de que al igual que el Muestreador de Gibbs, utiliza una estructura subyacente o de variables latentes.\\
\\
A pesar de reconocer la utilidad del algoritmo EM, el m\'etodo num\'erico que se utilizar\'a para la estimaci\'on en este trabajo ser\'a el Muestreador de Gibbs, el cual se explicar\'a con m\'as detalle en el capi\'itulo siguiente. De manera general, tomando la verosimilitud extendida definida anteriormente se fijan los valores iniciales para los par\'ametros $\alpha_d^{(0)},\alpha_\theta^{(0)},\beta_\theta^{(0)},\alpha_\gamma^{(0)},\beta_\gamma^{(0)}$ y para las variables latentes $\{\theta_i^{(0)}\}_{i=1}^{N(t)},\{\gamma_i^{(0)}\}_{i=1}^{N(t)}$ y para cada $k=1,...,N(t)$ tenemos la siguiente distribuci\'on que es proporcional a la verosimilitud,
\begin{align*}
\pi(\alpha_d^{(k)},\alpha_\theta^{(k)},\beta_\theta^{(k)},\alpha_\gamma^{(k)},\beta_\gamma^{(k)}|\{\theta_i^{(k-1)}\}_{i=2}^{N(t)},\{\gamma_i^{(k-1)}\}_{i=2}^{N(t)},\{d_i,c_i\}_{i=1}^{N(t)})
\end{align*}
Este es el principio necesario para utilizar el Muestreador de Gibbs de modo que se estimen los par\'ametros en base a las variables latentes que a su vez se estiman en base a las observaciones para que con los par\'ametros estimados se estimen las variables latentes que ayuden a predecir futuras observaciones. %Una vez que se tiene el concepto general de la aplicaci\'on de esta t\'ecnica de muestreo para el objetivo de este trabajo, es necesario considerar las maneras de implementarlo.\\