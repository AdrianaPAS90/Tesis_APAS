\chapter{Slice Sampler}
En la funci\'on de verosimilitus definida en la secci\'on anterior, las distribuciones de la variable latente $\gamma_i$ y de los par\'ametros $\alpha_d,\alpha_\theta,\alpha_\gamma$ no se pueden expresar de una forma anal\'itica cerrada. Es por esto que antes de empezar a estimar la funci\'on de verosimilitud es necesario muestrear primero estas distribuciones para lograr que los m\'etodos de estimaci\'on funcionen de la mejor manera. Este primer acercamiento se hace mediante el Slice Sampler.\\
\\
De acuerdo con \cite{neal2003slice} existen varias aplicaciones para esta manera de muestrear, desde la manera m\'as simple con una distribuci\'on univariada como alternativa al Muestreador de Gibbs hasta la m\'as compleja donde se pueden adaptar relaciones de dependencia entre las variables, permitiendo mayor flexibilidad al Muestreador de Gibbs. Esta es la clase de Slice Sampler que se utilizar\'a en este trabajo de investigaci\'on.\\
\\
La idea detr\'as del concepto del Slice Sampler es que si queremos muestrear la distribuci\'on de una variable $x$ que toma valores en el conjunto $\mathbb{R}^n$, cuya densidad sea proporcional a alguna funci\'on $f(x)$, entonces se muestrea de forma uniforme en la regi\'on por debajo de la curva de $f(x)$ con dimensi\'on $(n+1)$. Esta idea se puede concretar introduciendo una variable real auxiliar $y$ y definiendo la distribuci\'on conjunta de $x$ y $y$ que es uniforme sobre la regi\'on debajo de la curva de $f(x)$, tal que, $U=\{(x,y):0<y<f(x)\}$. Es decir, que para muestrear $x$, muestreamos conjuntamente $(x,y)$ para despu\'es ignorar $y$.\\
\\
Sin embargo, dado que generar puntos independientes muestreados de la distribuci\'on $U$ no siempre es sencillo, se puede definir una Cadena de Markov que converja a esta distribuci\'on uniforme. Esto se hace mediante el muestreo alternado de la distribuci\'on condicional de $y|x$ la cual es uniforme en el intervalo $(0,f(x))$ y de la distribuci\'on condicional de $x|y$ la cual tambi\'en es uniforme sobre la regi\'on $S=\{x_y<f(x)\}$, la cual es nombrada como la $rebanada$ definida por $y$. El procedimiento para construir la Cadena de Markov para una distribuci\'on univariada $f(x)$, tomando el valor inicial $x_0$, de acuerdo con \cite{neal2003slice}, es:
\begin{itemize}
\item Se extrae un valor real $y$ uniforme en $(0,f(x))$, definiendo una $rebanada$ horizontal $S=\{x:y<f(x)\}$. Es importante notar que $x_0$ est\'a siempre dentro de $S$.
\item Se encuentra un intervalo $I=(L,R)$ en la vecindad de $x_0$ que contenga toda, o la mayor parte, de la $rebanada$.
\item Se extrae un nuevo punto, $x_1$, de la parte de la $rebanada$ que est\'a dentro del intervalo $I$.
\end{itemize}
En el primer paso del procedimiento se elige la variable auxiliar que es caracter\'istica al Slice Sampler, pues este valor no es necesario de una iteraci\'on de la Cadena de Markov a la siguiente; mientras que los siguientes dos pasos pueden ser implementados de muchas maneras en tanto que la Cadena de Markov resultante permita que la distribuci\'on definida $f(x)$ permanezca invariante. El siguiente problema que se enfrenta es delimitar el intervalo, pues necesita ser lo suficientemente grande para que el nuevo punto ($x_1$) est\'e lo m\'as lejos del anterior ($x_0$) dentro de la misma $rebanada$, pero el intervalo tampoco puede salirse de la misma pues eso volver\'ia ineficiente al muestreo.\\
\\
Para garantizar convergencia en la distribuci\'on $f(x)$ y su invarianza, la Cadena de Markov debe ser erg\'odica. Seg\'un \cite{neal2003slice}, para demostrar que la funci\'on de distribuci\'on permanece invariante, suponemos que el estado inicial $x_0$ se distribuye $f(x)$, en el primer paso del procedimiento la distribuci\'on conjunta es con las variables $x_0$ y $y$, por lo que al actualizar $x_0$ a $x_1$ la funci\'on de distribuci\'on conjunta se mantiene invariante, ignorando as\'i la variable $y$. La distribuci\'on de $x_1$ es la distribuci\'on marginal de la conjunta, es decir, la funci\'on $f(x)$ definida. De este modo, lo \'unico que se necesita demostrar que la selecci\'on de $x_0$ y $x_1$ en los siguientes dos pasos del procedimiento deja a la distribuci\'on conjunta de $x$ y $y$ invariante, y con la distribuci\'on condicional sobre $S=\{x:y<f(x)\}$, es decir, la $rebanada$ definida por $y$. Esta invarianza se puede demostrar si la probabilidad de que $x_1$ sea el pr\'oximo estado dado que el estado actual es $x_0$ es igual a la probabilidad de que $x_0$ sea el pr\'oximo estado dado que el estado actual es $x_1$, para cualquier $x_0$ y $x_1$ en $S$.\\
\\
Este es el procedimiento que aplica para cuando la distribuci\'on es univariada, sin embargo, si esta llegara a ser una distribuci\'on multivariada ($x=(x_1,...,x_n)$) existen dos caminos para hacer el Slice Sampler: el primero es muestrear para cada variable por separado; para esto es necesario poder calcular la funci\'on $f_i(x_i)$ y que esta sea proporcional a $p(x_i|\{x_j\}_{j \neq i})$, donde $\{x_j\}_{j \neq i}$ son los valores de las variables. El otro camino es seguir el mismo procedimiento que se defini\'o para las distribuciones univariadas, solamente que en el segundo paso se reemplaza el intervalo con un hiperrect\'angulo $H=\{x:L_i<x_i<R_i, \quad i=1,...,n\}$ donde $L_i$ y $R_i$ definen la extensi\'on del hiperrect\'angulo a lo largo del eje para la variable $x_i$.\\
\\
En el caso de las distribuciones de los par\'ametros $\alpha_d,\alpha_\theta,\alpha_\gamma$ y de la variable latente $\gamma_i$ son distribuciones con una sola variable, por lo que el Slice Sampler a utilizar para estimar es el de una sola variable. Ahora, una vez estimado estas distribuciones es necesario utilizar el Muestreador de Gibbs que se desarrolla en la siguiente secci\'on para la estimaci\'on del modelo general de probabilidad.